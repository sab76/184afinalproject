{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe8b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SN_FILTER = True\n",
    "WINDOW = 4\n",
    "MAX_SEQ = 457\n",
    "d_model = 256\n",
    "num_layers = 12\n",
    "num_heads = 8\n",
    "dff = 256 * 2\n",
    "emb_rate = 0.4\n",
    "enc_rate = 0.4\n",
    "vocab_size = 425\n",
    "encoded_padded_value = 0\n",
    "\n",
    "class positional_encoding_layer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, maxlen, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_emb = self.positional_encoding(maxlen, d_model)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-2]\n",
    "        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.d_model, tf.float32)))\n",
    "        return x + self.pos_emb[0, :maxlen, :]\n",
    "\n",
    "    def positional_encoding(self, maxlen, d_model):\n",
    "\n",
    "        def get_angles(pos, i, d_model):\n",
    "            angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "            return pos * angle_rates\n",
    "\n",
    "        angle_rads = get_angles(np.arange(maxlen)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
    "        \n",
    "        return pos_encoding\n",
    "\n",
    "def create_encoder():\n",
    "    inp = tf.keras.layers.Input(shape=(None, ))\n",
    "    \n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size + 1, output_dim=d_model, mask_zero=True)(inp)\n",
    "    x = tf.keras.layers.Dropout(emb_rate)(inputs=x)\n",
    "    x = positional_encoding_layer(vocab_size=vocab_size, maxlen=MAX_SEQ, d_model=d_model)(x)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        \n",
    "        attn_output, _ = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(query=x, key=x, value=x, return_attention_scores=True)\n",
    "        attn_output = tf.keras.layers.Dropout(enc_rate)(attn_output)\n",
    "        mha_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        \n",
    "        ffn_output = tf.keras.layers.Dense(dff, activation='relu')(mha_output)\n",
    "        ffn_output = tf.keras.layers.Dense(d_model, activation='relu')(ffn_output)\n",
    "        ffn_output = tf.keras.layers.Dropout(enc_rate)(ffn_output)\n",
    "        \n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(mha_output + ffn_output)\n",
    "        \n",
    "    x = tf.keras.layers.Dense(d_model / 2, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(d_model / 4, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(d_model / 8, activation='relu')(x)\n",
    "    out = tf.keras.layers.Dense(2, activation='relu')(x)\n",
    "       \n",
    "    model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "    return model\n",
    "\n",
    "encoder_model = create_encoder()\n",
    "\n",
    "weights_path = 'model_weights.h5'\n",
    "encoder_model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b19e6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('subset_dataset_val.pkl', 'rb') as file:\n",
    "    val_data = pd.read_pickle(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87f7246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 11s 139ms/step\n",
      "Loss for the batch: 0.20739539\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.20769311\n",
      "64/64 [==============================] - 9s 137ms/step\n",
      "Loss for the batch: 0.20890361\n",
      "64/64 [==============================] - 9s 137ms/step\n",
      "Loss for the batch: 0.20996273\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.20755301\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.20893237\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.21049546\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.20913363\n",
      "64/64 [==============================] - 9s 135ms/step\n",
      "Loss for the batch: 0.20763488\n",
      "64/64 [==============================] - 9s 135ms/step\n",
      "Loss for the batch: 0.2095863\n",
      "64/64 [==============================] - 9s 135ms/step\n",
      "Loss for the batch: 0.20821513\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.20859042\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.20755245\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.2070042\n",
      "64/64 [==============================] - 9s 135ms/step\n",
      "Loss for the batch: 0.21007942\n",
      "64/64 [==============================] - 9s 137ms/step\n",
      "Loss for the batch: 0.20807658\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.20968083\n",
      "64/64 [==============================] - 9s 137ms/step\n",
      "Loss for the batch: 0.2079119\n",
      "64/64 [==============================] - 9s 138ms/step\n",
      "Loss for the batch: 0.20778066\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.20850232\n",
      "64/64 [==============================] - 9s 136ms/step\n",
      "Loss for the batch: 0.20836556\n",
      "64/64 [==============================] - 9s 135ms/step\n",
      "Loss for the batch: 0.21011715\n",
      "64/64 [==============================] - 9s 135ms/step\n",
      "Loss for the batch: 0.20976798\n"
     ]
    }
   ],
   "source": [
    "def masked_mae_loss(react_true, react_pred):\n",
    "    mask = tf.math.is_finite(react_true)\n",
    "    react_true_masked = tf.boolean_mask(react_true, mask)\n",
    "    react_pred_masked = tf.boolean_mask(react_pred, mask)\n",
    "    loss = tf.keras.losses.mean_absolute_error(react_true_masked, react_pred_masked)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "for input_data, react_true in val_data: \n",
    "    react_pred = encoder_model.predict(input_data)\n",
    "    loss = masked_mae_loss(react_true, react_pred)\n",
    "    print(\"Loss for the batch:\", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bc10c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
